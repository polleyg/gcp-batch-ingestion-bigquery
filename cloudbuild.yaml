steps:
  # 1. Fetch the source code
- name: gcr.io/cloud-builders/git
  args: ['clone', 'https://github.com/polleyg/gcp-batch-ingestion-bigquery.git']

  # 2a. Set up GCS & BQ etc. using Docker
- name: hashicorp/terraform
  args: ['init']
  dir: 'terraform'

  # 2b. Set up GCS & BQ etc. using Docker
- name: hashicorp/terraform
  args: ['apply', '-auto-approve']
  dir: 'terraform'
  id: terraform-apply

  # 3. Build and run the Dataflow pipeline (staged template)
- name: gcr.io/cloud-builders/gradle
  args: ['build', 'run']
  waitFor: ['terraform-apply']

  # 4a. Install npm & run tests
- name: gcr.io/cloud-builders/npm
  args: ['install', 'test']
  id: npm-install-test
  waitFor: ['terraform-apply']

  # 4b. Deploy the Cloud Function that listens to the bucket
- name: gcr.io/cloud-builders/gcloud
  args: ['functions', 'deploy', 'goWithTheDataFlow', '--stage-bucket=gs://batch-pipeline', '--trigger-bucket=gs://batch-pipeline']
  dir: 'cloud-function'
  waitFor: ['npm-install-test']

  # 5. Copy tarball to GCS to use later
artifacts:
  objects:
    location: 'gs://batch-pipeline/artifacts'
    paths: ['build/distributions/*.*']